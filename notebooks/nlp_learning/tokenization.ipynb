{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39da445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load('en_core_web_sm')\n",
    "fr_nlp = spacy.load('fr_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b34bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "sample\n",
      "sentence\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "en_test = \"This is a sample sentence.\"\n",
    "en_document = en_nlp(en_test)\n",
    "\n",
    "for token in en_document:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b2787a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci\n",
      "est\n",
      "une\n",
      "phrase\n",
      "de\n",
      "test\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "fr_test = \"Ceci est une phrase de test.\"\n",
    "fr_document = fr_nlp(fr_test)\n",
    "\n",
    "for token in fr_document:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b21da82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:Datasets\\wmt14_fr_en\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a654397024f54d3696d923de57e433b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "\n",
    "data_path = os.path.join(\"F:\", \"Datasets\", \"wmt14_fr_en\")\n",
    "if os.path.exists(data_path):\n",
    "    print(data_path)\n",
    "wmt14_en_fr_dataset = load_from_disk(dataset_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9245cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'en': 'Spectacular Wingsuit Jump Over Bogota', 'fr': 'Spectaculaire saut en \"wingsuit\" au-dessus de Bogota'}}\n"
     ]
    }
   ],
   "source": [
    "print(wmt14_en_fr_dataset['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c121459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Language(Enum):\n",
    "    EN = 0\n",
    "    FR = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9527a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(sentence, lang: int = Language.EN.value):\n",
    "    if lang:\n",
    "        return [token.text.lower() for token in fr_nlp.tokenizer(sentence)]\n",
    "    else:\n",
    "        return [token.text.lower() for token in en_nlp.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2dae2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def build_vocab(language_sentences, tokenizer, lang:int = Language.EN.value, min_freq: int = 2):\n",
    "    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "    counter = Counter()\n",
    "    \n",
    "    if lang == Language.EN.value:\n",
    "        for sentence in tqdm(language_sentences, desc=\"Building vocab for English\"):\n",
    "            counter.update(tokenizer(sentence['translation']['en'], lang))\n",
    "    else:\n",
    "        for sentence in tqdm(language_sentences, desc=\"Building vocab for French\"):\n",
    "            counter.update(tokenizer(sentence['translation']['fr'], lang))\n",
    "    \n",
    "    for word, freq in counter.items():\n",
    "        if freq > min_freq and word not in vocab:\n",
    "            ind = len(vocab)\n",
    "            vocab[word] = ind\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "093aede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = wmt14_en_fr_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dc6e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr_sentences = [data['translation']['fr'] for data in train_dataset]\n",
    "# en_sentences = [data['translation']['en'] for data in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ff764bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(fr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23da5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(en_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ace8338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:Datasets\\wmt14_fr_en_processed\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "processed_data_path = os.path.join(\"F:\", \"Datasets\", \"wmt14_fr_en_processed\")\n",
    "if os.path.exists(processed_data_path):\n",
    "    print(processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e571ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7df88c56b546989a8905a3e4adf37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocab for English:   0%|          | 0/40836715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_train_en = build_vocab(language_sentences=train_dataset, tokenizer=tokenize_en, lang=Language.EN.value)\n",
    "vocab_en_path = os.path.join(processed_data_path, \"vocab_en.pkl\")y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "322de382",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_en_path, \"wb\") as f:\n",
    "    pickle.dump(vocab_train_en, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "defd2cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e19f96770a442996e2411ea940cbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building vocab for French:   0%|          | 0/40836715 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_train_fr = build_vocab(language_sentences=train_dataset, tokenizer=tokenize_en, lang=Language.FR.value)\n",
    "vocab_fr_path = os.path.join(processed_data_path, \"vocab_fr.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e674917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_fr_path, \"wb\") as f:\n",
    "    pickle.dump(vocab_train_fr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "304752f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fr_path = os.path.join(processed_data_path, \"vocab_fr.pkl\")\n",
    "vocab_en_path = os.path.join(processed_data_path, \"vocab_en.pkl\")\n",
    "\n",
    "with open(vocab_fr_path, \"rb\") as f:\n",
    "    vocab_train_fr = pickle.load(f)\n",
    "with open(vocab_en_path, \"rb\") as f:\n",
    "    vocab_train_en = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aec4ee39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066763\n",
      "1142327\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_train_en))\n",
    "print(len(vocab_train_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2346a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\NLP_LLM_Coding\\venv\\Lib\\site-packages\\dill\\_dill.py:414: PicklingWarning: Cannot locate reference to <enum 'Language'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "e:\\NLP_LLM_Coding\\venv\\Lib\\site-packages\\dill\\_dill.py:414: PicklingWarning: Cannot pickle <enum 'Language'>: __main__.Language has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1638836687054447a5eb3e8c123cf3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40836715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "def preprocess(sample):\n",
    "    fr_sentence = sample['translation']['fr']\n",
    "    en_sentence = sample['translation']['en']\n",
    "    \n",
    "    fr_ids = [vocab_train_fr['<sos>']] + encode(tokenize_en(fr_sentence, lang = Language.FR.value), vocab_train_fr) + [vocab_train_fr['<eos>']]\n",
    "    en_ids = [vocab_train_en['<sos>']] + encode(tokenize_en(en_sentence, lang = Language.EN.value), vocab_train_en) + [vocab_train_en['<eos>']]\n",
    "    \n",
    "    return {'fr_ids': fr_ids, 'en_ids': en_ids}\n",
    "\n",
    "train_encoded_data = train_dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f603d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TranslationData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # super().__init__(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fr_indices = self.data[index]['fr_ids']\n",
    "        en_indices = self.data[index]['en_ids']\n",
    "        \n",
    "        src = torch.tensor(fr_indices, dtype=torch.long)\n",
    "        trg = torch.tensor(en_indices, dtype=torch.long)\n",
    "        return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_torch = TranslationData(train_encoded_data)\n",
    "train_data_torch_path = os.path.join(processed_data_path, 'training_fr_en_encoded_data.pt')\n",
    "torch.save(train_data_torch, train_data_torch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a47f82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    print(type(batch))\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=0, batch_first=True)\n",
    "    trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, padding_value=0, batch_first=True)\n",
    "    \n",
    "    return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4e4cc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([32, 75]) torch.Size([32, 56])\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train_data_torch, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for src, trg in data_loader:\n",
    "    print(src.shape, trg.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075c079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
